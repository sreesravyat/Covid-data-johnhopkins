{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0sC+35Oxm11r8SCOS6jKA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreesravyat/Covid-data-johnhopkins/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_bxGpmEJqCbr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_path = '/content/glove.6B.100d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space', 'rec.sport.hockey']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "texts, labels = newsgroups.data, newsgroups.target\n",
        "\n",
        "# Tokenize the texts\n",
        "tokens = [simple_preprocess(text) for text in texts]\n",
        "\n",
        "# Convert words to GloVe indices\n",
        "def doc2ind(doc, word_to_idx):\n",
        "    return [word_to_idx[word] if word in word_to_idx else 0 for word in doc]\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(glove_embeddings.keys(), 1)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "# Convert tokens to indices\n",
        "ng_vector_idx = [doc2ind(doc, word_to_idx) for doc in tokens]\n",
        "\n",
        "# Padding sequences to the same length\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "ng_vector_idx_padded = pad_sequence([torch.tensor(seq) for seq in ng_vector_idx], batch_first=True)\n",
        "labels = torch.tensor(labels)\n"
      ],
      "metadata": {
        "id": "Tfj6Nbn_qvBA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming glove_embeddings is a dictionary with word keys and vector values\n",
        "# and word_to_idx is a dictionary mapping words to their corresponding indices\n",
        "\n",
        "embedding_dim = 100  # Size of GloVe vectors, adjust if needed\n",
        "embedding_matrix = np.zeros((len(word_to_idx) + 1, embedding_dim))\n",
        "\n",
        "for word, idx in word_to_idx.items():\n",
        "    vector = glove_embeddings.get(word)\n",
        "    if vector is not None and len(vector) == embedding_dim:\n",
        "        embedding_matrix[idx] = vector\n",
        "    else:\n",
        "        print(f\"Word '{word}' not found in GloVe or has incorrect dimensions. Using random vector.\")\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "# Convert to torch tensor\n",
        "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "# Create an embedding layer\n",
        "glove_emb = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n"
      ],
      "metadata": {
        "id": "-_wo8iMHqvEk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, embedding_layer, num_classes):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = embedding_layer\n",
        "        self.fc1 = nn.Linear(100, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.mean(x, dim=1)  # Average pooling over sequence length\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "num_classes = len(categories)\n",
        "model = TextClassificationModel(glove_emb, num_classes)\n"
      ],
      "metadata": {
        "id": "iFrtXhW9qvF5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(ng_vector_idx_padded, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "accuracy = correct / len(test_loader.dataset)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvINPH5EqvIV",
        "outputId": "77768ebf-77a8-4dcc-c203-e64fa741b416"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.6141899824142456\n",
            "Epoch 2/10, Loss: 1.6055608987808228\n",
            "Epoch 3/10, Loss: 1.5987313985824585\n",
            "Epoch 4/10, Loss: 1.6064560413360596\n",
            "Epoch 5/10, Loss: 1.5835797786712646\n",
            "Epoch 6/10, Loss: 1.5882816314697266\n",
            "Epoch 7/10, Loss: 1.571858286857605\n",
            "Epoch 8/10, Loss: 1.5631811618804932\n",
            "Epoch 9/10, Loss: 1.5398235321044922\n",
            "Epoch 10/10, Loss: 1.5482674837112427\n",
            "Test Accuracy: 0.3292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the embedding layer\n",
        "model.embedding.weight.requires_grad = True\n",
        "\n",
        "# Fine-tune the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Fine-Tuning Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Re-evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "fine_tuned_accuracy = correct / len(test_loader.dataset)\n",
        "print(f'Fine-Tuned Test Accuracy: {fine_tuned_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzLZw9TPqvK5",
        "outputId": "4fff1444-d5b7-4b1a-e2a0-afe5a0220d76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuning Epoch 1/10, Loss: 1.5280665159225464\n",
            "Fine-Tuning Epoch 2/10, Loss: 1.5403600931167603\n",
            "Fine-Tuning Epoch 3/10, Loss: 1.525551676750183\n",
            "Fine-Tuning Epoch 4/10, Loss: 1.5483684539794922\n",
            "Fine-Tuning Epoch 5/10, Loss: 1.5164172649383545\n",
            "Fine-Tuning Epoch 6/10, Loss: 1.526739239692688\n",
            "Fine-Tuning Epoch 7/10, Loss: 1.4913585186004639\n",
            "Fine-Tuning Epoch 8/10, Loss: 1.4941202402114868\n",
            "Fine-Tuning Epoch 9/10, Loss: 1.5412604808807373\n",
            "Fine-Tuning Epoch 10/10, Loss: 1.4814260005950928\n",
            "Fine-Tuned Test Accuracy: 0.4522\n"
          ]
        }
      ]
    }
  ]
}