{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMH9PtdgSvyzEqXWIr0RZEF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreesravyat/Covid-data-johnhopkins/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n"
      ],
      "metadata": {
        "id": "5J_wzIdJZDHz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "\n",
        "glove_path = '/content/glove.6B.100d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "mhOyY_24YutZ",
        "outputId": "c022e0e0-db06-4e90-c8d8-73bacdc3756d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/glove.6B.100d.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-37e5062fea8a>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mglove_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/glove.6B.100d.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mglove_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-37e5062fea8a>\u001b[0m in \u001b[0;36mload_glove_embeddings\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_glove_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/glove.6B.100d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from gensim.utils import simple_preprocess\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Fetch data\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.politics.mideast', 'rec.sport.baseball']\n",
        "ng_data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Tokenize\n",
        "tokens = [simple_preprocess(text) for text in ng_data.data]\n",
        "\n",
        "# Create word to index mapping\n",
        "def doc2ind(doc, word2index):\n",
        "    return [word2index[word] for word in doc if word in word2index]\n",
        "\n",
        "# Create a mapping of words to indices\n",
        "word2index = {word: idx for idx, word in enumerate(glove_embeddings.keys(), 1)}\n",
        "\n",
        "# Convert documents to indices\n",
        "ng_vector_idx = [torch.tensor(doc2ind(doc, word2index)) for doc in tokens]\n",
        "\n",
        "# Pad sequences to ensure they are of the same length\n",
        "ng_vector_idx = pad_sequence(ng_vector_idx, batch_first=True, padding_value=0)\n"
      ],
      "metadata": {
        "id": "td3NjYhRZBNt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Create GloVe matrix\n",
        "embedding_dim = 100\n",
        "glove_matrix = np.zeros((len(word2index) + 1, embedding_dim))\n",
        "for word, idx in word2index.items():\n",
        "    glove_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "# Define the embedding layer\n",
        "glove_emb = nn.Embedding.from_pretrained(torch.FloatTensor(glove_matrix))\n",
        "glove_emb.weight.requires_grad = False  # Initially, keep the embeddings fixed\n"
      ],
      "metadata": {
        "id": "mLZHB3ddZBQC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_layer, num_classes):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = embedding_layer\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(128 * 2, num_classes)  # 2 for bidirectional\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "num_classes = len(categories)\n",
        "model = TextClassifier(glove_emb, num_classes)\n"
      ],
      "metadata": {
        "id": "DflCzRCdZBTg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Prepare data for training\n",
        "labels = torch.tensor(ng_data.target)\n",
        "dataset = TensorDataset(ng_vector_idx, labels)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train with frozen embeddings\n",
        "for epoch in range(5):\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Unfreeze the embeddings and fine-tune\n",
        "model.embedding.weight.requires_grad = True\n",
        "for epoch in range(5):\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+6}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "6LoJIdmkbCce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G2Mr9zgxd-Um"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}